### **EAL-параметры**

| Параметр | Описание | Пример | Примечание |
|----------|----------|--------|------------|
| **`-l <ядра>`** | Логические ядра CPU (через запятую или диапазон) | `-l 0-3,5` | Обязательный параметр |
| **`-n <каналы>`** | Число каналов памяти | `-n 4` | Обычно равно числу слотов RAM |
| **`-a <PCI_ADDR>`** | Привязать PCI-устройство | `-a 0000:01:00.0` | Можно несколько раз |
| **`--vdev=<драйвер>`** | Добавить виртуальное устройство | `--vdev=net_af_xdp0,iface=eth0` | Для AF_XDP, PCAP и др. |
| **`--socket-mem=<МБ>`** | Память на NUMA-узлах (через запятую) | `--socket-mem=1024,0` | Для NUMA-систем |
| **`--socket-limit=<МБ>`** | Лимит памяти на узлах | `--socket-limit=2048,2048` | |
| **`--file-prefix=<строка>`** | Префикс для файлов hugepages | `--file-prefix=app1` | Для нескольких DPDK-приложений |
| **`--no-pci`** | Отключить PCI | `--no-pci` | Для виртуальных устройств |
| **`--log-level=<уровень>`** | Уровень логирования (`debug`, `info`, `error`) | `--log-level=pmd:debug` | |
| **`--iova-mode=<pa/va>`** | Режим IOVA (Physical/Virtual Addressing) | `--iova-mode=pa` | По умолчанию `va` |

### 1. `-l <ядра>` / `--lcores`
> **Описание:** Указывает, какие **логические ядра процессора (lcore)** будут использоваться DPDK-приложением.

#### Пример:
```bash
-l 0-3,5
```

#### Что делает:
- Запускает приложение только на указанных ядрах.
- Ядра нумеруются от `0` до `n`.
- Можно задавать как диапазоны (`0-3`), так и отдельные номера (`5`).

#### Влияние:
- Улучшает производительность за счёт предсказуемого распределения нагрузки.
- Позволяет выделить определённые ядра под обработку RX/TX/обработку пакетов.
- Может уменьшить contention между потоками.

#### Когда использовать:
- При работе с многопоточностью.
- Для NUMA-aware приложений.
- Чтобы не мешать другим системным процессам.

---

### 2. `-n <каналы>` / `--memory-channels`
> **Описание:** Число **каналов памяти**, доступных в системе.

### Что такое каналы памяти?
Архитектура любого компьютера подразумевает наличие процессора и оперативной памяти. Причем память непосредственно связана с процессором через специальный контроллер. По сути оперативную память можно рассматривать как дополнительный уровень кеша, вынесенного за пределы ЦП с большим объемом и низкой скоростью работы в сравнении с интегрированной в процессор памятью. Обмен данными между современными процессорами и «оперативкой» может осуществляться в одно- или двухканальном режиме. О том, что такое каналы памяти и как их количество влияет на производительность мы и поговорим в этой статье.

Канал памяти, по сути, представляет собой поток данных, посредством которого происходит обмен информацией между процессором и оперативной памятью. Количество каналов памяти определяет скорость обмена информацией. Причем сам канал физически присутствует в компьютере, в виде контроллера особой архитектуры и прямой связи с планками памяти в виде медных дорожек в текстолите материнской платы.

Современные десктопные процессоры оснащаются контроллерами, в которых имеется два канала памяти. Работают эти каналы параллельно, то есть их пропускная способность складывается. Важно учитывать, что количество каналов памяти никак не влияет на количество планок, которое поддерживает процессор или материнская плата. Хотя эти параметры связаны, но не совсем напрямую.

Даже процессор с двумя каналами памяти может работать в одноканальном режиме. Этот режим активируется при условии, что установлена только одна планка памяти. Но даже если пользователь использует две планки, это не гарантирует, что оба канала памяти активируются. Для этого их нужно подключить так, чтобы каждая была соединена с отдельным каналом памяти.

Если в материнской плате присутствуют только два слота, то сложностей не возникнет и при использовании двух планок оба канала памяти активируются автоматически. Если же имеется 4 слота, то планки нужно ставить через слот, если установить их рядом друг с другом, то обе окажутся подключены к одному каналу памяти и второй останется незадействованным.

То есть в двухканальный процессор можно установить до 4 планок «оперативки», по две на каждый канал. Проверить, в каком режиме работает контроллер можно в биос или в мониторинговых программах.
## Влияние на производительность

В эпоху, когда самые производительные процессоры оснащались 4 ядрами, количество активированных каналов памяти было не слишком важно. Но сейчас ситуация радикально поменялась, так как количество ядер процессора увеличилось в несколько раз. И все они обмениваются данными через все те же два канала памяти. Поэтому сейчас важно приобретать оперативную память комплектом по 2 или 4 планки и подключать их таким образом, чтобы были задействованы оба канала памяти процессора.

За счет этого радикально (практически вдвое) возрастает скорость обмена данными и повышается общая производительность компьютера. Увеличение производительности обусловлено тем, что процессорные ядра меньше простаивают в ожидании поступления данных из оперативной памяти.

Нужно учитывать, что хотя при включении двух каналов памяти скорость обмена данными почти удваивается, далеко не всегда и не во всех программах производительность существенно возрастает. В среднем увеличение производительности составляет 10-20%, в некоторых программах и играх прирост будет почти незаметным, в других – он может составить даже больше 50%.
#### Пример:
```bash
-n 4
```

#### Что делает:
- Определяет, сколько физических каналов памяти используется.
- Используется для оптимизации доступа к памяти.

#### Влияние:
- Влияет на производительность работы с hugepages.
- Неправильный выбор может привести к снижению пропускной способности.

#### Когда использовать:
- Настройка зависит от конкретной системы.
- Обычно равно числу слотов RAM или контроллеров памяти.

---

### 3. `-a <PCI_ADDR>` / `--allow`
> **Описание:** Разрешает использование указанного PCI-устройства.
#### Пример:
```bash
-a 0000:01:00.0
```

#### Что делает:
- Указывает, какие сетевые карты (NIC) могут быть использованы приложением.
- Можно указать несколько устройств, повторяя параметр.

#### Влияние:
- Предотвращает случайное использование других устройств.
- Полезно при множестве NIC.

#### Когда использовать:
- Если нужно выбрать конкретную карту.
- В системах с несколькими сетевыми адаптерами.

#### ## Как узнать, какие устройства можно использовать?
```bash
lspci | grep Eth
```

---

### 4. `--vdev=<драйвер>`
> **Описание:** Добавляет **виртуальное устройство** (не связанное с реальным PCI-устройством).

`--vdev` — это **EAL-параметр (Environment Abstraction Layer)**, который позволяет **создавать виртуальные Poll Mode Drivers (PMD)** без привязки к реальному PCI-устройству.

Это похоже на создание "мок" (mock) сетевых устройств, которые могут быть использованы как обычные порты в DPDK-приложениях, таких как `dpdk-testpmd`, `dpdk-pdump`, `OVS-DPDK`, `VPP` и т.д.

---

#### Синтаксис

```bash
--vdev "<driver_name>[,<key=value>,...]"
```

- `<driver_name>` — имя драйвера виртуального устройства.
- После запятой — опциональные параметры в формате `ключ=значение`.

---

#### Примеры использования `--vdev` в `dpdk-testpmd`

### 1. **Создание виртуального TAP-устройства**
```bash
./dpdk-testpmd -l 0-3 --socket-mem=1024 --vdev=net_tap0 -- \
                --portmask=0x1 --forward-mode=mac
```

- Здесь создаётся виртуальное TAP-устройство с именем `net_tap0`.
- Оно будет отображаться как порт `0` в `testpmd`.
- Можно отправлять/принимать пакеты через него из пользовательского пространства или из ядра.

---

### 2. **Создание нескольких виртуальных устройств**
```bash
./dpdk-testpmd -l 0-3 --socket-mem=1024 \
               --vdev=net_tap0 --vdev=net_tap1 -- \
               --portmask=0x3 --forward-mode=mac
```

- Создаются два виртуальных порта: `0` и `1`.
- Они будут связаны с `net_tap0` и `net_tap1`.

---

### 3. **Создание AF_XDP устройства**
```bash
./dpdk-testpmd -l 0-3 --socket-mem=1024 \
               --vdev=net_af_xdp0,iface=lo -- \
               --portmask=0x1 --forward-mode=mac
```

- Используется для работы с XDP через AF_XDP.
- Порт будет работать поверх `loopback` интерфейса.

---

### 4. **Создание PCAP устройства**
```bash
./dpdk-testpmd -l 0-3 --socket-mem=1024 \
               --vdev=net_pcap0,rx_pcap=input.pcap,tx_pcap=output.pcap -- \
               --portmask=0x1 --forward-mode=mac
```

- Приём пакетов из файла `input.pcap`.
- Отправка пакетов в файл `output.pcap`.

---

### Поддерживаемые типы виртуальных устройств

| Тип устройства | Описание |
|----------------|----------|
| `net_tap`      | Виртуальный TAP-интерфейс, доступный в ядре Linux. Полезен для связи между DPDK и внешним миром. |
| `net_af_xdp`   | Работа с интерфейсами через XDP и AF_XDP (eBPF). Высокая производительность. |
| `net_pcap`     | Чтение/запись пакетов из/в pcap-файл. Полезно для тестирования и воспроизведения трафика. |
| `net_ring`     | Внутреннее кольцо (ring), используемое для обмена пакетами между lcore'ами или между приложениями. |
| `net_vhost`    | Vhost-user клиент/сервер. Полезно для связи с QEMU/KVM или другим vSwitch. |
| `net_null`     | Фиктивное устройство, не передающее пакеты. Используется для тестирования и нагрузки CPU. |
| `net_memif`    | Memory Interface — высокопроизводительный интерфейс между процессами или VM. |

### Как устроено виртуальное устройство внутри?

Каждое `--vdev`:
- Создаёт **виртуальный PMD (Poll Mode Driver)**.
- Устройство регистрируется в EAL как полноценный порт.
- Может иметь RX/TX очереди, буферы, статистику и прочее.
- Обрабатывает пакеты в user-space, как и физическое устройство.

> Для DPDK-приложений виртуальное устройство выглядит **ровно так же, как и физическое PCI-устройство**.

---

### Как взаимодействовать с виртуальными устройствами?

#### 1. **TAP-устройства**
После запуска `testpmd` с `--vdev=net_tap0`:
```bash
ip link show tap_net_tap0
```

Можно назначить IP-адрес:
```bash
sudo ip addr add 192.168.0.1/24 dev tap_net_tap0
sudo ip link set dev tap_net_tap0 up
```

Теперь ты можешь пинговать его:
```bash
ping 192.168.0.1
```

---

#### 2. **AF_XDP**
Используется для работы с eBPF/XDP. Может быть полезен для:
- Высокопроизводительного фильтра пакетов,
- Балансировки нагрузки,
- Инлайн-обработки трафика.

---

#### 3. **PCAP**
Для тестирования:
- Запись входящего трафика в pcap-файл.
- Воспроизведение трафика из pcap-файла.

---

#### 4. **Ring**
Работает как внутренняя очередь, например:
```bash
--vdev=net_ring0
```

Можно использовать для связи между двумя частями одного приложения или даже между разными DPDK-приложениями.

---

#### Расширенная настройка

#### Передача параметров

Например, для `net_tap` можно указать MTU и другие свойства:
```bash
--vdev=net_tap0,mtu=9000,mac=00:11:22:33:44:55
```

Для `net_vhost`:
```bash
--vdev=vhost_user0,iface=/tmp/vhost-sock0,queues=2,tso=1
```

## ✅ Преимущества использования `--vdev`

| Преимущество | Описание |
|-------------|----------|
| Нет зависимости от железа | Можно запускать DPDK-приложения без физических NIC |
| Легко тестировать | Воспроизводить трафик из pcap, эмулировать поведение |
| Возможность межпроцессного взаимодействия | Через `net_ring`, `memif`, `vhost` |
| Подходит для CI/CD и автоматизации | Автоматические тесты без аппаратной зависимости |
| Гибкость | Поддержка множества типов виртуальных устройств |

## ❌ Ограничения

| Ограничение | Описание |
|------------|----------|
| Не всегда максимальная производительность | Например, `net_tap` медленнее физического PMD |
| Ограниченная функциональность | Не все функции поддерживаются (например, hardware offload) |
| Нужны дополнительные зависимости | Например, libpcap-dev для `net_pcap` |

---

### 5. `--socket-mem=<МБ>`
> **Описание:** Выделяет память (hugepages) на каждом NUMA-сокете.

#### Пример:
```bash
--socket-mem=1024,0
```

#### Что делает:
- Указывает, сколько памяти выделяется на каждом NUMA-узле.
- В примере: 1024 МБ на первом сокете, 0 на втором.

#### Влияние:
- Важно для NUMA-aware приложений.
- Уменьшает задержки при обращении к локальной памяти.

#### Когда использовать:
- В многопроцессорных системах с NUMA.
- Для точного контроля над памятью.

---

### 6. `--socket-limit=<МБ>`
> **Описание:** Устанавливает **лимит памяти** на NUMA-сокетах.

#### Пример:
```bash
--socket-limit=2048,2048
```

#### Что делает:
- Защищает от перерасхода памяти.
- Если попытаться выделить больше, чем разрешено — будет ошибка.

#### Влияние:
- Полезно для предотвращения OOM (Out Of Memory).
- Может помочь в тестировании и отладке.

#### Когда использовать:
- При ограничении ресурсов.
- Для защиты от ошибок в коде.

---

### 7. `--file-prefix=<строка>`
> **Описание:** Префикс для файлов hugepages и других временных файлов.

#### Пример:
```bash
--file-prefix=app1
```

#### Что делает:
- Все файлы, создаваемые DPDK (например, `/dev/hugepages/rte_map_*`), будут иметь префикс `app1_`.
- Это позволяет запускать **несколько DPDK-приложений одновременно**.

#### Влияние:
- Изолирует пространство имён между приложениями.
- Позволяет избежать конфликтов.

#### Когда использовать:
- При параллельном запуске нескольких DPDK-приложений.

---

### 8. `--no-pci`
> **Описание:** Отключает работу с PCI-устройствами.

#### Пример:
```bash
--no-pci
```

#### Что делает:
- Не инициализирует PCI-устройства.
- Полезно при использовании виртуальных драйверов.

#### Влияние:
- Ускоряет запуск приложений без физических NIC.
- Снижает зависимости от оборудования.

#### Когда использовать:
- Для тестовых приложений.
- При использовании `--vdev`.

---

### 9. `--log-level=<уровень>`
> **Описание:** Устанавливает уровень логирования.

#### Пример:
```bash
--log-level=pmd:debug
```

#### Что делает:
- Контролирует вывод логов по компонентам.
- Возможные уровни: `emergency`, `alert`, `critical`, `error`, `warning`, `notice`, `info`, `debug`.

#### Влияние:
- Debug-логи полезны для отладки.
- Высокий уровень логирования может влиять на производительность.

#### Когда использовать:
- При отладке проблем с драйверами.
- Для понимания внутреннего состояния приложения.

---

### 10. `--iova-mode=<pa/va>`
> **Описание:** Режим адресации IOVA (Input/Output Virtual Address).

#### Пример:
```bash
--iova-mode=pa
```

#### Что делает:
- `pa` — Physical Addressing: использует физические адреса для DMA.
- `va` — Virtual Addressing: использует виртуальные адреса (по умолчанию).

#### Влияние:
- PA требует IOMMU/VT-d, но работает быстрее.
- VA более гибкий, но может вызывать проблемы с некоторыми драйверами.

---

### **Параметры testpmd**
| Параметр                            | Описание                                                                                                                                 | Пример                                                    | Примечание                                                                                     |
| ----------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------- | ---------------------------------------------------------------------------------------------- |
| **`-i`**                            | Включить интерактивный режим                                                                                                             | `-- -i`                                                   | Без этого флага запускается автономный режим                                                   |
| **`--portmask=<маска>`**            | Маска портов (hex)                                                                                                                       | `--portmask=0x3`                                          | Порты 0 и 1                                                                                    |
| **`--rxq=<N>`**                     | Число RX-очередей на порт                                                                                                                | `--rxq=4`                                                 | Зависит от NIC                                                                                 |
| **`--txq=<N>`**                     | Число TX-очередей на порт                                                                                                                | `--txq=4`                                                 |                                                                                                |
| **`--rxd=<N>`**                     | Размер RX-очереди (дескрипторы)                                                                                                          | `--rxd=2048`                                              | Увеличить при потерях пакетов                                                                  |
| **`--txd=<N>`**                     | Размер TX-очереди (дескрипторы)                                                                                                          | `--txd=2048`                                              |                                                                                                |
| **`--burst=<N>`**                   | Пакетов за одну обработку                                                                                                                | `--burst=64`                                              | Оптимально: 32-128                                                                             |
| **`--mbcache=<N>`**                 | Кэш mbuf                                                                                                                                 | `--mbcache=512`                                           |                                                                                                |
| **`--forward-mode=<режим>`**        | Режим пересылки: `io`, `mac`, `macswap`, `flowgen`, `rxonly`, `txonly`                                                                   | `--forward-mode=mac`                                      |                                                                                                |
| **`--eth-peer=<порт,MAC>`**         | Установить MAC адрес                                                                                                                     | `--eth-peer=0,00:11:22:33:44:55`                          | Для L2-форвардинга                                                                             |
| **`--stats-period=<сек>`**          | Период вывода статистики                                                                                                                 | `--stats-period=5`                                        |                                                                                                |
| **`--tx-first`**                    | Начать с передачи пакетов                                                                                                                | `--tx-first`                                              | Для тестирования TX                                                                            |
| **`--disable-hw-vlan`**             | Отключить аппаратную обработку VLAN                                                                                                      | `--disable-hw-vlan`                                       |                                                                                                |
| **`--rss-ip`**                      | Включить RSS для IP-трафика                                                                                                              | `--rss-ip`                                                | Требует поддержки NIC                                                                          |
| **`--max-pkt-len=<байт>`**          | Максимальный размер пакета                                                                                                               | `--max-pkt-len=9000`                                      | Для Jumbo Frames                                                                               |
| **`--nb-cores=<количество>`**       | Количество ядре задействованых в работе                                                                                                  | **`--nb-cores=24`**                                       |                                                                                                |
| **`--main-lcore=<число>`**          | Назначает главное ядро                                                                                                                   | **`--main-lcore=23`**                                     |                                                                                                |
| **`--total-num-mbufs=N`**           | установить количество mbuf, выделяемых в пулах mbuf.                                                                                     | **`--total-num-mbufs=262144`**                            |                                                                                                |
| **`--numa`**                        | включить распределение колец RX/TX и буферов памяти RX (mbufs) с учетом NUMA.                                                            |                                                           |                                                                                                |
| **`--mbuf-size=N[, N1[, N2 ...]]`** | позволяет настраивать размеры буферов (mbuf), используемых для приёма и обработки сетевых пакетов                                        | **`--mbuf-size=2048`**\|\|**`--mbuf-size=128,2048,9018`** |                                                                                                |
| **`--disable-rss`**                 | Отключает **RSS (Receive Side Scaling)** – механизм распределения сетевого трафика между несколькими RX-очередями с помощью хеширования. |                                                           |                                                                                                |
| **`--num-procs=N`**                 | установить общее количество экземпляров многопроцессорных процессов                                                                      | **`--num-procs=8`**                                       |                                                                                                |
| **`--no-numa`**                     | отключить распределение с поддержкой NUMA                                                                                                |                                                           |                                                                                                |
| **`--mbcache=N`**                   | управляет **кэшированием mbuf (memory buffers)** в DPDK, что напрямую влияет на производительность обработки пакетов                     | **`--mbcache=256`**                                       | Параметр `--mbcache=N` задаёт **количество mbuf, заранее зарезервированных для каждого ядра**. |
| **`--multi-rx-mempool`**            | включить поддержку multi-rx-mempool                                                                                                      |                                                           |                                                                                                |
| **`--mlockall`**                    | блокирует **всю память процесса DPDK в физической RAM**, запрещая её выгрузку в swap                                                     |                                                           |                                                                                                |
| **`--no-mlockall`**                 | отключает эту блокировку (по умолчанию DPDK использует `mlockall`)                                                                       |                                                           |                                                                                                |
| **`--mp-alloc <method>`**           | способ выделения памяти для пулов (mempool)                                                                                              |                                                           | смотри ниже возмодные значения                                                                 |

#### **Основные параметры testpmd**

#### **`-i` (Interactive Mode)**
- **Назначение**: Включает интерактивный режим управления.
- **Влияние**: Без этого флага `testpmd` запускается в автономном режиме (только с предустановленными настройками).
- **Пример**: 
  ```bash
  -- -i
  ```

#### **`--portmask=<маска>`**
- **Назначение**: Определяет, какие порты будут использоваться (битовая маска).
- **Влияние**: 
  - `0x1` – только порт 0
  - `0x3` – порты 0 и 1 (биты `0b11`)
  
В DPDK термин **"порт"** соответствует **одному сетевому устройству или его части** , которое способно:
- Получать пакеты (RX),
- Отправлять пакеты (TX),
- Обрабатывать определённый тип трафика.
Это **абстракция над физическим или виртуальным сетевым интерфейсом** .
#### 1. Физические порты сетевой карты (NIC)
Большинство современных сетевых карт (например, Intel X710, Mellanox ConnectX) имеют **по нескольку портов**
###### Пример:
Сетевая карта Intel X710:
- Имеет **2 физических порта** .
- Каждый из них может быть подключен к разным коммутаторам.
- В системе они видны как отдельные PCI-устройства или как один функциональный блок с несколькими PCI-функциями (например, `02:00.0` и `02:00.1`)

**Как это выглядит в `testpmd`:**
```bash
./dpdk-testpmd -l 0-3 --socket-mem=1024 \
               -a 02:00.0 -a 02:00.1 -- \
               --portmask=0x3 --forward-mode=mac
```

- **Пример**:
  ```bash
  --portmask=0x3
  ```

#### **`--rxq=<N>`, `--txq=<N>`**

Эти параметры управляют количеством **очередей приема (`rxq`)** и **передачи (`txq`)** в сетевых интерфейсах (особенно актуально для высокопроизводительных сетевых карт, таких как Intel NIC, DPDK, virtio-net и др.).
#### **Receive Queues (`rxq`) – Очереди приема**
- Это **буферы (очереди)**, в которые попадают входящие пакеты до обработки CPU или драйвером.
- Чем больше очередей, тем лучше распределяется нагрузка между ядрами CPU (особенно важно для многопоточных систем).
- Используется механизм **RSS (Receive Side Scaling)** для балансировки нагрузки.

#### **Transmit Queues (`txq`) – Очереди передачи**
- Это **буферы (очереди)**, в которые складываются исходящие пакеты перед отправкой в сеть.
- Больше очередей → лучше параллелизм при передаче (меньше конкуренции за доступ к NIC).
- Особенно важно для серверов с высокой исходящей нагрузкой (например, веб-серверы, VPN, стриминг).
#### **Проблемы при неправильной настройке:**

- **Мало очередей (`rxq=1`, `txq=1`)** → Узкое горлышко, CPU не может эффективно распределять нагрузку.
- **Слишком много очередей** → Лишние накладные расходы (особенно если CPU мало).
- **Несоответствие числу CPU** → Если очередей меньше, чем ядер, часть CPU будет простаивать.

#### **Оптимальные значения:**
Обычно **количество очередей = количеству CPU-ядер** (или близко к этому).
Например, для сервера с 16 ядрами:
```bash
--rxq=16 --txq=16
```
Но иногда лучше **меньше очередей**, если нагрузка невысокая (например, `rxq=4`, `txq=4`).

#### **`--rxd=<N>`, `--txd=<N>`**

Эти параметры управляют **размерами (глубиной) очередей дескрипторов** для приема (`rxd`) и передачи (`txd`) в сетевых интерфейсах. Они критически важны для производительности, особенно в высоконагруженных системах (например, серверы, DPDK, виртуализация).

### **Receive Descriptors (`rxd`) – Дескрипторы приема**
- Это **кольцевой буфер (ring buffer)**, где хранятся указатели на пакеты, ожидающие обработки CPU.
- Каждый дескриптор содержит информацию о пакете (адрес в памяти, длину, статус).
- Чем больше `rxd`, тем больше пакетов может быть принято до переполнения.

### **Transmit Descriptors (`txd`) – Дескрипторы передачи**
- Аналогично, это буфер для пакетов, которые ожидают отправки в сеть.
- Больший размер позволяет накапливать больше данных перед передачей.

### **Проблемы при неправильной настройке:**
- **Слишком маленький `rxd`/`txd`:**
    - **Потеря пакетов** (drops в `ifconfig`/`ethtool -S`).
    - **Рост задержек** (пакеты ждут освобождения дескрипторов).
    - **Снижение пропускной способности** (NIC не успевает обрабатывать данные).
    
- **Слишком большой `rxd`/`txd`:**
    - **Увеличение потребления памяти** (каждый дескриптор занимает место).
    - **Рост latency** (пакеты дольше находятся в буфере перед обработкой).
    - **Не всегда дает прирост производительности** (если нагрузка небольшая).

### **Оптимальные значения:**
- Зависят от:
    - **Нагрузки** (10G/25G/100G сети требуют больше дескрипторов).
    - **Задержки** (low-latency системы используют меньшие буферы).
    - **Драйвера/NIC** (некоторые карты имеют ограничения, например, 1024 или 4096).
    
- **Рекомендуемые стартовые значения:**
    - Для 10G: `rxd=1024`, `txd=1024`.
    - Для 25G/100G: `rxd=2048`, `txd=2048`.
    - Для low-latency: `rxd=256`, `txd=256`.

- **Назначение**: Размер очередей (число дескрипторов).
- **Влияние**:
  - `rxd` – буферизация входящих пакетов.
  - `txd` – буферизация исходящих пакетов.
  - При малых значениях возможны потери пакетов.
- **Пример**:
  ```bash
  --rxd=2048 --txd=2048
  ```

#### **`--burst=<N>`**
- **Назначение**: Количество пакетов, обрабатываемых за один цикл.
- **Влияние**:
  - Оптимальные значения: 32–128.
  - Большие значения снижают нагрузку на CPU, но увеличивают задержку.
- **Пример**:
  ```bash
  --burst=64
  ```

#### **`--mbcache=<N>`**
- **Назначение**: Размер кэша для mbuf (пакетных буферов).
- **Влияние**:
  - Уменьшает частоту запросов к пулу памяти.
  - Рекомендуется: 256–1024.
- **Пример**:
  ```bash
  --mbcache=512
  ```


#### **`--mp-alloc <метод>` — способ выделения памяти для пулов (mempool)**

| Метод        | Память для создания пула | Память для данных (пакетов) | Когда использовать?                           |
| ------------ | ------------------------ | --------------------------- | --------------------------------------------- |
| **native**   | DPDK-память (hugepages)  | DPDK-память                 | Стандартный режим, лучшая производительность  |
| **anon**     | DPDK-память              | Анонимная память (malloc)   | Тесты, если не хватает hugepages              |
| **xmem**     | Анонимная память         | Анонимная память            | Отладка без hugepages                         |
| **xmemhuge** | Анонимные hugepages      | Анонимные hugepages         | Альтернатива `native` без DPDK-резервирования |

### **Ключевые различия**
1. **`native`**
    
    - Использует **только предварительно выделенные hugepages** (через `--huge-dir`)
    - Максимальная производительность (память зарезервирована заранее).
2. **`anon`**
    - Пулы создаются в DPDK, но **данные пакетов — в обычной памяти**.
    - Медленнее, но требует меньше hugepages.
3. **`xmem`**
    - Всё выделяется динамически через `malloc`.
    - **Не рекомендуется для продакшена** (риск фрагментации памяти).
4. **`xmemhuge`**
    - Пытается использовать hugepages "на лету" (если доступны).
    - Компромисс между гибкостью и скоростью.

---

#### **Режимы работы**
#### **`--forward-mode=<режим>`**
- **Назначение**: Выбор алгоритма пересылки пакетов.
- **Режимы**:
  - `io` – простой форвардинг (без изменений пакета).
  - `mac` – L2-форвардинг с подменой MAC-адресов.
  - `macswap` – обмен MAC-адресов местами.
  - `flowgen` – генерация тестового трафика.
  - `rxonly` – только приём пакетов.
  - `txonly` – только передача.
- **Пример**:
  ```bash
  --forward-mode=mac
  ```

#### **`--eth-peer=<порт,MAC>`**
- **Назначение**: Установка MAC-адреса для порта.
- **Влияние**:
  - Используется в режимах `mac`/`macswap`.
  - Позволяет эмулировать L2-соседей.
- **Пример**:
  ```bash
  --eth-peer=0,00:11:22:33:44:55
  ```

---

#### **Диагностика и настройка**
#### **`--stats-period=<сек>`**
- **Назначение**: Периодичность вывода статистики.
- **Влияние**:
  - При `0` статистика выводится только при остановке.
  - При `5` – каждые 5 секунд.
- **Пример**:
  ```bash
  --stats-period=5
  ```

#### **`--tx-first`**
- **Назначение**: Начать работу с передачи пакетов.
- **Влияние**:
  - Полезно для тестирования TX-пути.
  - В обычном режиме сначала идёт приём.
- **Пример**:
  ```bash
  --tx-first
  ```

#### **`--disable-hw-vlan`**
- **Назначение**: Отключает аппаратную обработку VLAN.
- **Влияние**:
  - VLAN-теги обрабатываются программно.
  - Снижает производительность, но полезно для отладки.
- **Пример**:
  ```bash
  --disable-hw-vlan
  ```

#### **`--rss-ip`**
- **Назначение**: Включает RSS (Receive Side Scaling) для IP-трафика.
- **Влияние**:
  - Распределяет нагрузку по ядрам CPU.
  - Требует поддержки со стороны NIC.
- **Пример**:
  ```bash
  --rss-ip
  ```

#### **`--max-pkt-len=<байт>`**
- **Назначение**: Максимальный размер пакета.
- **Влияние**:
  - Для Jumbo Frames устанавливается 9000+.
  - Должен совпадать с настройками сети.
- **Пример**:
  ```bash
  --max-pkt-len=9000
  ```

---

### **Параметры для виртуальных устройств (через `--vdev`)**
| Драйвер | Параметры | Пример | Описание |
|---------|-----------|--------|----------|
| **`net_af_xdp`** | `iface=<интерфейс>`, `queue_count=<N>` | `--vdev=net_af_xdp0,iface=eth0,queue_count=4` | Для XDP-пакетов |
| **`net_pcap`** | `rx_pcap=<файл>`, `tx_pcap=<файл>` | `--vdev=net_pcap0,rx_pcap=input.pcap` | Чтение/запись PCAP |
| **`net_tap`** | `iface=<имя>`, `mac=<адрес>` | `--vdev=net_tap0,iface=tap0` | Виртуальный TAP-интерфейс |
| **`net_vhost`** | `iface=<путь>`, `client=<0/1>` | `--vdev=net_vhost0,iface=/tmp/vhost.sock` | Для QEMU/KVM |

---

### **Параметры NUMA**
| Параметр                             | Описание                     | Пример                                           |
| ------------------------------------ | ---------------------------- | ------------------------------------------------ |
| **`--port-numa-config=<порт,узел>`** | Привязка порта к NUMA-узлу   | `--port-numa-config=0,1` (порт 0 на numa-node 1) |
| **`--ring-numa-config=<порт,узел>`** | Привязка ring-буферов к узлу | `--ring-numa-config=0,1`                         |
| **`--socket-num=<узел>`**            | Основной NUMA-узел           | `--socket-num=1`                                 |
### **1. --port-numa-config**
Привязывает **порт** (в вашем случае — виртуальное устройство `net_af_xdp`) к определённому NUMA-узлу.

**Формат**:  
`(port_id, numa_node)`

**Например `--port-numa-config='(0,1)'`:
- `port_id=0` (первый и единственный порт).
- `numa_node=1` — порт привязан к NUMA-узлу **1** (второй сокет).

**Зачем нужно**:
- Гарантирует, что RX/TX-очереди порта будут использовать память из NUMA-узла `1`, что критично для производительности, если NIC физически подключена к этому узлу.
- Если NIC на узле `1`, а порт привязан к узлу `0`, возможны значительные задержки из-за межсокетного доступа к памяти.

### **2. --ring-numa-config**
Управляет размещением **кольцевых буферов** (ring buffers), которые связывают RX/TX-очереди портов с ядрами обработки.

**Формат**:  
`(port_id, ring_type, numa_node)`
- `ring_type`:
    - `0` = RX-кольцо (приём).
    - `1` = TX-кольцо (передача).
    - `3` = **И RX, и TX** (оба типа).

**Например `--ring-numa-config='(0,3,1)'`**:
- `port_id=0` (первый порт).
- `ring_type=3` — настройка применяется и к RX, и к TX-кольцам.
- `numa_node=1` — кольцевые буферы размещаются на NUMA-узле **1**.

**Зачем нужно**:
- Оптимизирует доступ ядер к очередям. Если ядра обрабатывающие пакеты, находятся на том же NUMA-узле (`1`), задержки минимальны.
- В вашей команде ядра `0-23` находятся на узле `0`, а `40-47` — на узле `1`. Возможно, часть ядер обращается к удалённой памяти (через межсокетную шину), что объясняет разницу в производительности между очередями.

Эти параметры критически важны для оптимизации производительности в системах с **NUMA-архитектурой** (многосокетные серверы). Они управляют распределением ресурсов между узлами NUMA, минимизируя задержки доступа к памяти.

### **3. --socket-num**
Указывает DPDK, на каком NUMA-узле (socket) размещать **основные структуры данных** и **память по умолчанию**.

**Подробности**:
- В вашей системе два NUMA-узла: `0` (первый сокет) и `1` (второй сокет).
- Этот параметр говорит DPDK выделять память для внутренних структур (например, очередей, буферов) на NUMA-узле **1**.
- Влияет на производительность, если ядра приложения работают на том же узле, что и память (снижает задержки доступа).

**Пример**:  
Если NIC (сетевая карта) подключена к NUMA-узлу `1`, а `--socket-num=0`, возможны лишние межсокетные задержки при доступе к данным.

---

#### **1. Основы NUMA (Non-Uniform Memory Access)**
- В многосокетных системах каждый CPU-сокет имеет **локальную память** (быстрый доступ) и **удаленную память** (медленный доступ, через межсокетную шину).
- Пример для 2-сокетного сервера:
  ```
  NUMA Node 0: CPU 0-15, RAM 0-64GB
  NUMA Node 1: CPU 16-31, RAM 64-128GB
  ```
- **Проблема**: Если процесс на Node 0 использует память из Node 1, задержки возрастают на 30-50%.

---

#### **2. Параметры NUMA и их влияние**

#### **`--port-numa-config=<порт,узел>`**
- Жестко привязывает сетевой порт к конкретному NUMA-узлу.
- Все обработка трафика этого порта будет выполняться на CPU и памяти указанного узла.
#### **Зачем нужно?**
- Оптимизирует доступ к памяти:
  - Драйвер NIC и буферы пакетов размещаются в локальной памяти.
  - Уменьшаются задержки межсокетного доступа.
- Особенно важно для:
  - 10G/25G/100G сетей.
  - DPDK-приложений.
  - Виртуализации (например, VM с vCPU на определенном узле).

#### **Пример:**
```bash
--port-numa-config=0,1   # Порт 0 привязан к NUMA Node 1
```

#### **Как проверить привязку портов к NUMA?**
```bash
lspci -vvv | grep -i numa
ethtool -i eth0 | grep numa_node
```

#### **`--ring-numa-config=<порт,узел>`**
- Указывает, на каком NUMA-узле размещаются **кольцевые буферы (ring buffers)** для приема/передачи пакетов.
- Ring-буферы — это области памяти, куда NIC пишет пакеты (RX) и откуда читает (TX).
#### **Зачем нужно?**
- Если кольцевые буферы находятся в том же узле, что и CPU-ядра обработчики:
  - Снижается задержка доступа к буферам.
  - Увеличивается пропускная способность.
- Актуально для:
  - Высокопроизводительных сетей (DPDK, Solarflare, Mellanox).
  - Low-latency систем (HFT, VoIP).

#### **Пример:**
```bash
--ring-numa-config=0,1   # Ring-буферы порта 0 на Node 1
```

#### **Как проверить текущее размещение ring-буферов?**
```bash
cat /sys/class/net/eth0/device/numa_node
```

#### **`--socket-num=<узел>`**
#### **Что делает?**
- Задает **основной NUMA-узел** для всего приложения.
- Все потоки и память по умолчанию будут выделяться на этом узле.

#### **Зачем нужно?**
- Гарантирует, что:
  - Память выделяется локально (не через межсокетную шину).
  - Потоки работают на CPU указанного узла.
- Упрощает управление ресурсами в многопоточных приложениях.

#### **Пример:**
```bash
--socket-num=1   # Основной узел — Node 1
```

#### **Как проверить текущий NUMA-узел процесса?**
```bash
numactl --hardware
taskset -cp <PID>
```

---

## **3. Как эти параметры влияют на производительность?**
### **Сценарии:**
1. **Правильная настройка** (все ресурсы на одном узле):
   - Порт 0 → Node 1.
   - Ring-буферы → Node 1.
   - Потоки обработки → Node 1.
   - **Результат**: Минимальные задержки, максимум throughput.

2. **Неправильная настройка** (ресурсы на разных узлах):
   - Порт 0 → Node 0.
   - Ring-буферы → Node 1.
   - **Результат**: Задержки из-за доступа к удаленной памяти, потери пакетов.

3. **Без настройки NUMA**:
   - Ядра и память могут распределяться случайно.
   - **Результат**: Непредсказуемые задержки, просадки производительности.

---

## **4. Практические рекомендации**
### **1. Определите топологию NUMA**
```bash
numactl --hardware
```
Вывод:
```
available: 2 nodes (0-1)
node 0 cpus: 0-15
node 0 memory: 64 GB
node 1 cpus: 16-31
node 1 memory: 64 GB
```

### **2. Привяжите NIC к NUMA-узлу**
- Узнайте, к какому узлу относится сетевой интерфейс:
  ```bash
  cat /sys/class/net/eth0/device/numa_node
  ```
- Если `-1`, NIC не имеет явной привязки (используйте `ethtool` или BIOS).

### **3. Запустите приложение с правильными параметрами**
Пример для DPDK:
```bash
./my_app --port-numa-config=0,1 --ring-numa-config=0,1 --socket-num=1
```

### **4. Мониторинг**
- Проверяйте загрузку CPU и память по узлам:
  ```bash
  numastat -z
  ```
- Следите за потерями пакетов:
  ```bash
  ethtool -S eth0 | grep drop
  ```

---

## **5. Оптимизации для конкретных случаев**
### **Для DPDK:**
- Используйте `--lcores` для привязки потоков к ядрам:
  ```bash
  --lcores=1-16@16   # Потоки 1-16 на CPU 16-31 (Node 1)
  ```
- Настройте hugepages на нужном узле:
  ```bash
  echo 1024 > /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages
  ```

**Пример для 2-сокетного сервера с 10G NIC:**
```bash
./network_app \
  --port-numa-config=0,1 \
  --ring-numa-config=0,1 \
  --socket-num=1 \
  --lcores=1-8@16
```

---

## **Команды интерактивного режима**
После `-- -i`:
```bash
testpmd> start      # Запустить пересылку
testpmd> stop       # Остановить
testpmd> show port stats all  # Статистика
testpmd> set fwd mac  # Сменить режим
testpmd> set burst 128  # Изменить burst size
```

---

### **Полный список параметров**
Для просмотра всех доступных опций:
```bash
./dpdk-testpmd --help       # EAL-параметры
./dpdk-testpmd -- --help    # Параметры testpmd
```

---

### **🎯 Примеры команд**
1. **Физический NIC с 2 очередями:**
   ```bash
   sudo ./dpdk-testpmd -l 0-3 -n 4 -a 0000:01:00.0 -- \
     -i \
     --portmask=0x1 \
     --rxq=2 --txq=2 \
     --rxd=2048 --txd=2048 \
     --burst=64 \
     --forward-mode=mac
   ```

2. **AF_XDP с 4 очередями:**
   ```bash
   sudo ./dpdk-testpmd -l 0-3 -n 4 --no-pci \
     --vdev=net_af_xdp0,iface=eth0,queue_count=4 -- \
     -i \
     --forward-mode=rxonly
   ```

3. **Генерация трафика (Jumbo Frames):**
   ```bash
   sudo ./dpdk-testpmd -l 0-3 -n 4 -a 0000:01:00.0 -- \
     -i \
     --tx-first \
     --txpkts=9000 \
     --max-pkt-len=9000
   ```

Эта таблица покрывает 95% сценариев использования `dpdk-testpmd`. Для экзотических параметров (например, `--rss-hf`) см. официальную документацию.


Вот максимально простое и практичное руководство по работе с `dpdk-testpmd` в интерактивном режиме:

### 🔥 Основы работы
1. **Запуск** (обязательные параметры):
```bash
sudo ./dpdk-testpmd -l 0-3 -n 4 -- -i
```
Где:
- `-l 0-3` - ядра процессора (логические)
- `-n 4` - каналы памяти
- `-- -i` - вход в интерактивный режим

2. **Что видим после запуска**:
- Список доступных портов (NIC)
- Настройки очередей RX/TX
- Режим работы (по умолчанию `io` - простой форвардинг)

### 🛠️ Основные команды
| Команда | Что делает | Пример |
|---------|-----------|--------|
| `start` | Запустить пересылку пакетов | `start` |
| `stop` | Остановить пересылку | `stop` |
| `show port stats all` | Показать статистику | `show port stats all` |
| `set fwd <режим>` | Сменить режим работы | `set fwd mac` |
| `set promisc all on/off` | Включить/выключить promiscuous mode | `set promisc all on` |

### 🔄 Режимы работы
1. **io** (по умолчанию):
   - Простая пересылка пакетов между портами без изменений
   ```bash
   set fwd io
   ```

2. **mac**:
   - Форвардинг с подменой MAC-адресов
   ```bash
   set fwd mac
   set eth-peer 0,00:11:22:33:44:55
   ```

3. **macswap**:
   - Меняет местами MAC-адреса в пакетах
   ```bash
   set fwd macswap
   ```

### 📊 Полезные команды диагностики
1. Показать информацию о порте:
```bash
show port info 0
```

2. Показать информацию об очередях:
```bash
show rxq info 0 0  # Для порта 0, очереди 0
show txq info 1 0  # Для порта 1, очереди 0
```

3. Настройка размера burst (пакетов за раз):
```bash
set burst 64
```


> **MTU (Maximum Transmission Unit)** — это **параметр сетевого интерфейса** , который определяет **максимальный размер полезной нагрузки (payload)** , которую может передать или принять устройство за один раз.  
> Это **не часть заголовка пакета** , а **локальная настройка сетевой карты и стека TCP/IP** , которая влияет на:
> 
> - Размер буферов в памяти.
> - Максимальный размер пакета, с которым может работать система.
> - Производительность и надёжность приёма/передачи данных.
 Чтобы изменить **MTU (Maximum Transmission Unit)** в `dpdk-testpmd`, вы можете    использовать **встроенные команды интерфейса CLI**, предоставляемые `testpmd`.
> Чтобы изменить MTU в `testpmd`, используйте команду:
```bash
set port <port_id> mtu <value>
```

Затем перезапустите порт:
```bash
port stop <port_id>
port start <port_id>
```

---

### 1. **Вход в интерфейс `testpmd`**
После запуска `testpmd` вы попадаете в командную строку, где можно вводить команды.

Пример запуска:
```bash
sudo ./build/app/dpdk-testpmd -l 0-3 -n 4 --socket-mem 1024,1024 -- --portmask=0x1
```

Вы увидите приглашение:
```bash
testpmd>
```

---

### 2. **Установка нового MTU**

Если ты хочешь просто увидеть, какие порты доступны (без деталей), можно использовать:
```bash
show port summary all
```

Если нужно посмотреть конкретный порт:
```bash
show port info <port_id>
```

Используйте команду:
```bash
set port <port_id> mtu <new_mtu_value>
```

Например:
```bash
set port 0 mtu 9000
```

Это установит MTU равным **9000 байт** для порта `0`.

> ⚠️ После изменения MTU **необходимо остановить и снова запустить порт**, чтобы изменения вступили в силу.

Остановка и запуск порта:
```bash
port stop 0
port start 0
```

После этого новый MTU будет применён, и сетевая карта начнёт принимать пакеты размером до `<MTU + заголовки>`.

---

### 4. **Проверка текущего MTU**

Чтобы проверить текущее значение MTU:
```bash
show port info 0
```

В выводе вы увидите:
```text
MTU: 9000
```

---

## 🛠 Важные детали

### a) **Размер буферов должен быть достаточным**
Когда вы увеличиваете MTU (например, до 9000 для Jumbo Frames), убедитесь, что:
- Размер буфера (`mbuf_size`) достаточно велик, чтобы вместить пакет.
- Пул `mbuf` был создан с правильным размером.

Если не хватает места в буфере, сетевая карта будет отбрасывать большие пакеты.

---

### b) **Поддержка Jumbo Frames драйвером**
Не все драйверы поддерживают большие значения MTU. Убедитесь, что ваша сетевая карта поддерживает Jumbo Frames:
```bash
show port info 0
```

Ищите информацию о максимальной длине пакета:
```text
Max RX packet length: 9000
```

---

### c) **Дополнительная настройка (если нужно)**
Если вы хотите указать MTU **при запуске `testpmd`**, можно использовать параметр командной строки:
```bash
sudo ./build/app/dpdk-testpmd -l 0-3 -n 4 --socket-mem 1024,1024 -- --portmask=0x1 --mtu 9000
```

Также можно задать MTU через `--eth-peers-configfile` или другие механизмы инициализации.

- Если вы получаете ошибку типа `"failed to configure device"`, значит:
  - MTU слишком большой для драйвера.
  - Недостаточно большого размера буферов.
- Используйте `--total-numa-mem` и `--socket-mem` для выделения достаточной памяти.
- Для тестирования Jumbo Frames лучше использовать `txonly` или `rxonly` режимы.

---

## Дополнительные полезные команды

| `show port stats all`  | Показать статистику приёма/передачи пакетов     |
| ---------------------- | ----------------------------------------------- |
| `show port xstats all` | Показать расширенную статистику                 |
| `show config fwd`      | Показать текущую конфигурацию пересылки пакетов |
| `list pci`             | Показать все обнаруженные PCI-устройства        |

---


### 🚀 Пример рабочего сеанса
1. Запускаем testpmd:
```bash
sudo ./dpdk-testpmd -l 0-3 -n 4 -- -i
```

2. В интерактивном режиме:
```bash
set fwd mac
set burst 64
start

# Ждем 10 секунд
show port stats all
stop
```

### ⚠️ Частые проблемы
1. **Не видит порты**:
   - Проверьте PCI-адрес карты: `lspci | grep Ethernet`
   - Добавьте флаг `-a <PCI_ADDR>` при запуске

2. **Низкая производительность**:
   - Увеличьте burst: `set burst 128`
   - Добавьте очереди: `port config all rxq 2`

3. **Большие потери пакетов**:
   - Увеличьте размеры очередей:
   ```bash
   port config all rxd 2048
   port config all txd 2048
   ```

### 💡 Полезные советы
1. Автодополнение команд работает по Tab
2. История команд доступна стрелками вверх/вниз
3. Для выхода - `quit` или Ctrl+C

Это минимально необходимый набор для начала работы. Все остальные команды можно посмотреть через `help` в интерактивном режиме.

## Архитектура testpmd

```
+--------------------------------------------------+
|                  CLI / Commands                  |
+-------------------+------------------------------+
                    |
+-------------------v-------------------+     +-----------------------------+
|           Packet Forwarding           |     |        Port Management      |
| - simple_forwarding_runner()          |<--->| - start_port(), stop_port() |
| - burst RX/TX                         |     | - configure_rxtx_dump_cb()  |
| - callback hooks                      |     | - rx_queue_setup()          |
+---------------------------------------+     +-----------------------------+

+------------------+       +----------------+     +------------------------+
|    Mempool &     |<----->|   Mbuf Pool    |     | Ethdev (librte_ethdev) |
|    Mbuf Handling |       |                |     |                        |
+------------------+       +----------------+     +------------------------+

+---------------------------+
|         EAL Layer         |
| - rte_eal_init()          |
| - memory allocation       |
| - PCI/VDEV device probing |
+---------------------------+
```