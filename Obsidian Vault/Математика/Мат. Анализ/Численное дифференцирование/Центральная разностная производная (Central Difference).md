**Связанные темы:** [[Правая разностная производная (Forward Difference)]], [[Левая разностная производная (Backward Difference)]]

Если вы хотите вычислить приближённое значение производной функции \( f(x) \) в точке \( x = a \), то **центральная разностная производная** задаётся формулой:

$$
f'(a) \approx \frac{f(a + h) - f(a - h)}{2h},
$$

где:
- \( h \) — малый шаг (обычно \( h \ll 1 \)),
- \( f(a + h) \) — значение функции в точке \( a + h \),
- \( f(a - h) \) — значение функции в точке \( a - h \).

**Напоминаем формулу производной:**
$$
f'(a) = \lim_{h \to 0} \frac{f(a + h) - f(a)}{h},
$$

---

## **1. Почему центральная разность точнее?**
Центральная разность использует **симметричное приращение**, что даёт более точную аппроксимацию производной, чем левая или правая разности.

### **Сравнение с односторонними разностями:**
| Метод               | Формула              | Погрешность |
| ------------------- | -------------------- | ----------- |
| **Правая разность** | f(a+h) - f(a) / h    | O(h)        |
| **Левая разность**  | f(a) - f(a-h) / h    | O(h)        |
| **Центральная**     | f(a+h) - f(a-h) / 2h | O(h^2)      |

**Почему \( O(h^2) \)?**  
Разложение в ряд Тейлора вокруг \( a \):
$$ f(a + h) = f(a) + h f'(a) + \frac{h^2}{2} f''(a) + \frac{h^3}{6} f'''(a) + \dots $$
$$ f(a - h) = f(a) - h f'(a) + \frac{h^2}{2} f''(a) - \frac{h^3}{6} f'''(a) + \dots $$

Вычитаем второе из первого:
$$
f(a + h) - f(a - h) = 2h f'(a) + \frac{h^3}{3} f'''(a) + \dots
$$

Делим на \( 2h \):
$$
\frac{f(a + h) - f(a - h)}{2h} = f'(a) + \underbrace{\frac{h^2}{6} f'''(a) + \dots}_{\text{погрешность } O(h^2)}.
$$

**Вывод:** Главный член погрешности пропорционален `h^2`, поэтому метод точнее, чем односторонние разности. То есть, погрешность будет уменьшаться `h^2` а не на `h` как двух других методах при уменьшении `h`

---

## **2. Точность и выбор шага \( h \)**
- **Погрешность усечения** (теоретическая): \( O(h^2) \).
- **Погрешность округления**: при слишком малых \( h \) разность \( f(a+h) - f(a-h) \) теряет точность из-за вычитания близких чисел.

**Оптимальный шаг \( h \):**
$$
h \approx \epsilon^{1/3} \cdot \max(1, |a|),
$$
где \( \epsilon \) — машинная точность (~\( 10^{-16} \) для `double`).

**Пример для \( a = 1 \):**
$$
h \approx (10^{-16})^{1/3} \approx 10^{-5}.
$$

---

## **3. Пример вычисления**
**Функция:** \( f(x) = x^3 \)  
**Точка:** \( a = 1 \)  
**Точная производная:** \( f'(1) = 3x = 3 \).

**Численный расчёт при \( h = 0.01 \):**
$$
f'(1) \approx \frac{f(1.01) - f(0.99)}{0.02} = \frac{1.030301 - 0.970299}{0.02} = \frac{0.060002}{0.02} = 3.0001.
$$
**Погрешность:** \( 0.0001 \) (соответствует \( O(h^2) \)).

---

## **4. Когда использовать?**
✅ **Плюсы:**
- Высокая точность (\( O(h^2) \)).
- Хорошо работает для гладких функций.

❌ **Минусы:**
- Требует вычисления функции в двух точках (\( a + h \) и \( a - h \)).
- Менее устойчива к шумам, чем односторонние разности.

**Альтернативы для ещё большей точности:**
1. **5-точечная формула** (\( O(h^4) \)):
   $$
   f'(a) \approx \frac{-f(a+2h) + 8f(a+h) - 8f(a-h) + f(a-2h)}{12h}.
   $$
2. **Методы автоматического дифференцирования** (если функция аналитическая).

---

## **5. Код на Python**
```python
import numpy as np

def central_diff(f, x, h=1e-5):
    """Вычисляет центральную разностную производную."""
    return (f(x + h) - f(x - h)) / (2 * h)

# Пример: производная sin(x) в x = 0 (должна быть cos(0) = 1)
x = 0.0
f = np.sin
df_approx = central_diff(f, x)
df_exact = np.cos(x)

print(f"Приближённая производная: {df_approx}")
print(f"Точная производная: {df_exact}")
print(f"Ошибка: {abs(df_approx - df_exact)}")
```

**Вывод:**
```
Приближённая производная: 1.0000000000287557
Точная производная: 1.0
Ошибка: 2.875566451616419e-11
```
(Ошибка очень мала благодаря \( O(h^2) \).)

---

## **6. Вывод**
- **Центральная разностная производная** значительно точнее, чем левая/правая разности.
- Оптимальный шаг \( h \) зависит от машинной точности.
- Лучше подходит для гладких функций, но требует вычисления \( f \) в двух точках.
- В Python реализуется в несколько строк, как и другие методы численного дифференцирования.