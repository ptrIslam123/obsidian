
***Регрессия — способ выбрать из семейства функций ту, которая минимизирует функцию потерь. Последняя характеризует насколько сильно пробная функция отклоняется от значений в заданных точках. Если точки получены в эксперименте, они неизбежно содержат ошибку измерений, шум, поэтому разумнее требовать, чтобы функция передавала общую тенденцию, а не точно проходила через все точки. В каком-то смысле регрессия — это «интерполирующая аппроксимация»: мы хотим провести кривую как можно ближе к точкам и при этом сохранить ее максимально простой чтобы уловить общую тенденцию. За баланс между этими противоречивыми желаниями как-раз отвечает функция потерь (в английской литературе «loss function» или «cost function»).**

Словосочение линейная и регресия говорит о том что к вачестве функциии апроксимации будет взята линейная функция(Можно взять и дригие классы функции и тогда это уже будет не линейная регрессия).

Линейная регрессия моделируется следующей формулой:
# $$ y = a * x + b $$

где:

- ( y ) - выходная переменная (целевая переменная), которую мы пытаемся предсказать.
- ( a ) - коэффициент наклона, который показывает, насколько сильно выходная переменная изменяется при изменении входной переменной.
- ( x ) - входная переменная (признак), который мы используем для предсказания выходной переменной.
- ( b ) - константа, которая показывает, где линия регрессии пересекает ось ( y ).


**Целью линейной регрессии является минимизация ошибки прогноза. Ошибка в данном случае - это разница между предсказанным и фактическим значением. В линейной регрессии эта разница обычно измеряется с помощью метрики, такой как среднеквадратическая ошибка (MSE) или средняя абсолютная ошибка (MAE).
Минимизация этой ошибки достигается путем нахождения коэффициентов ( a ) и ( b ), которые минимизируют эту метрику. Это обычно достигается с помощью методов оптимизации, таких как градиентный спуск или методы, основанные на статистике, такие как метод наименьших квадратов.**


![[one_dim_lin_reg.png]]


Одним из простых методов для минимизации MSE является метод наименьших квадратов, который ищет коэффициенты ( a ) и ( b ), минимизирующие сумму квадратов отклонений между фактическими и предсказанными значениями.

Пример кода линейной регресии на python: [[линейная регрессия на python]]

>### Оценка результатов
> - **[[Коэффициент детерминации ( R^2 )]]**:  
    Коэффициент детерминации ( R^2 ) - это доля дисперсии зависимой переменной, объясняемая моделью. Он принимает значения от 0 до 1, где 1 означает, что модель полностью объясняет дисперсию, а 0 означает, что модель не объясняет дисперсию вообще.
> - [[Средняя абсолютная ошибка (MAE)]]:  
    Средняя абсолютная ошибка (MAE) - это среднее абсолютное отклонение прогнозируемых значений от фактических значений. Она менее чувствительна к выбросам, чем MSE.
> - [[Коэффициент корреляции Пирсона]]:  
    Коэффициент корреляции Пирсона показывает степень линейной связи между двумя переменными. В контексте регрессии, он может использоваться для оценки, насколько хорошо модель описывает зависимость между входными и выходными переменными.
> - [[**Коэффициенты регрессии**]]:  
    Коэффициенты регрессии (коэффициенты наклона и свободный член) дают информацию о том, насколько сильно выходная переменная изменяется при изменении входной переменной.
> - [[**p-значения для коэффициентов**]]:  
    p-значения для коэффициентов показывают, насколько статистически значимо различие между фактическими и прогнозируемыми значениями. Если p-значение маленькое (обычно меньше 0.05), то можно отклонить нулевую гипотезу о том, что коэффициент не значимо отличается от нуля.
> - [[**Графики остатков**]]:  
    Графики остатков (или ошибок) могут помочь визуально оценить качество модели. Если график остатков случайно, то модель хорошо описывает данные.
#### Связанные темы [[Интерполяция]], [[Экстропаляция]], [[Апроксимация]]
#### Ссылки https://habr.com/ru/articles/514818/
