***Регрессия — способ выбрать из семейства функций ту, которая минимизирует функцию потерь. Последняя характеризует насколько сильно пробная функция отклоняется от значений в заданных точках. Если точки получены в эксперименте, они неизбежно содержат ошибку измерений, шум, поэтому разумнее требовать, чтобы функция передавала общую тенденцию, а не точно проходила через все точки. В каком-то смысле регрессия — это «интерполирующая аппроксимация»: мы хотим провести кривую как можно ближе к точкам и при этом сохранить ее максимально простой чтобы уловить общую тенденцию. За баланс между этими противоречивыми желаниями как-раз отвечает функция потерь (в английской литературе «loss function» или «cost function»).**

Словосочение линейная и регресия говорит о том что к вачестве функциии апроксимации будет взята линейная функция(Можно взять и дригие классы функции и тогда это уже будет не линейная регрессия).

Линейная регрессия моделируется следующей формулой:
# $$ y = a * x + b $$

где:

- ( y ) - выходная переменная (целевая переменная), которую мы пытаемся предсказать.
- ( a ) - коэффициент наклона, который показывает, насколько сильно выходная переменная изменяется при изменении входной переменной.
- ( x ) - входная переменная (признак), который мы используем для предсказания выходной переменной.
- ( b ) - константа, которая показывает, где линия регрессии пересекает ось ( y ).


**Целью линейной регрессии является минимизация ошибки прогноза. Ошибка в данном случае - это разница между предсказанным и фактическим значением. В линейной регрессии эта разница обычно измеряется с помощью метрики, такой как среднеквадратическая ошибка (MSE) или средняя абсолютная ошибка (MAE).
Минимизация этой ошибки достигается путем нахождения коэффициентов ( a ) и ( b ), которые минимизируют эту метрику. Это обычно достигается с помощью методов оптимизации, таких как градиентный спуск или методы, основанные на статистике, такие как метод наименьших квадратов.**


![[one_dim_lin_reg.png]]


Одним из простых методов для минимизации MSE является метод наименьших квадратов, который ищет коэффициенты ( a ) и ( b ), минимизирующие сумму квадратов отклонений между фактическими и предсказанными значениями.

Пример кода линейной регресии на python: [[линейная регрессия на python]]

>### Оценка результатов
> - **[[Коэффициент детерминации ( R^2 )]]**:  
    Коэффициент детерминации ( R^2 ) - это доля дисперсии зависимой переменной, объясняемая моделью. Он принимает значения от 0 до 1, где 1 означает, что модель полностью объясняет дисперсию, а 0 означает, что модель не объясняет дисперсию вообще.
> - [[Средняя абсолютная ошибка (MAE)]]:  
    Средняя абсолютная ошибка (MAE) - это среднее абсолютное отклонение прогнозируемых значений от фактических значений. Она менее чувствительна к выбросам, чем MSE.
> - [[Коэффициент корреляции Пирсона]]:  
    Коэффициент корреляции Пирсона показывает степень линейной связи между двумя переменными. В контексте регрессии, он может использоваться для оценки, насколько хорошо модель описывает зависимость между входными и выходными переменными.
> - [[**Коэффициенты регрессии**]]:  
    Коэффициенты регрессии (коэффициенты наклона и свободный член) дают информацию о том, насколько сильно выходная переменная изменяется при изменении входной переменной.
> - [[**p-значения для коэффициентов**]]:  
    p-значения для коэффициентов показывают, насколько статистически значимо различие между фактическими и прогнозируемыми значениями. Если p-значение маленькое (обычно меньше 0.05), то можно отклонить нулевую гипотезу о том, что коэффициент не значимо отличается от нуля.
> - [[**Графики остатков**]]:  
    Графики остатков (или ошибок) могут помочь визуально оценить качество модели. Если график остатков случайно, то модель хорошо описывает данные.

## Пример
- **X** — случайные числа из некоторой области определения (например, от 0 до 10),
- **Y** — значение линейной функции вида $$Y = a \cdot X + b $$, где `a` и `b` — произвольные коэффициенты (например, `a = 2.5`, `b = 1.3`.

| №  |     X     |     Y      |
|----|-----------|------------|
| 1  |  3.47     |   9.98     |
| 2  |  1.23     |   4.38     |
| 3  |  7.82     |  20.85     |
| 4  |  0.56     |   2.70     |
| 5  |  5.01     |  13.83     |
| 6  |  9.12     |  24.10     |
| 7  |  2.34     |   7.15     |
| 8  |  4.56     |  12.70     |
| 9  |  6.78     |  18.25     |
| 10 |  8.90     |  23.55     |
| 11 |  0.12     |   1.60     |
| 12 |  3.33     |   9.63     |
| 13 |  7.00     |  18.80     |
| 14 |  2.10     |   6.55     |
| 15 |  5.55     |  15.18     |

Формула:  
$$
Y = 2.5 \cdot X + 1.3
$$

#### Задача линейной регрессии

Метод наименьших квадратов (МНК). Это стандартный метод для линейной регрессии. 
## 1. Задача: минимизация ошибки

Пусть у нас есть данные — пары `x_i`, `y_i`, где `i = 1, 2, ..., n`. Мы предполагаем, что зависимость между `x` и `y` линейная:

$$
\hat{y}_i = a x_i + b
$$

Цель — найти такие значения `a` и `b`, чтобы сумма квадратов отклонений между реальными `y_i` и предсказанными `y_i с крышечкой` была минимальна:

$$
S(a, b) = \sum_{i=1}^{n}(y_i - \hat{y}_i)^2 = \sum_{i=1}^{n}(y_i - a x_i - b)^2 \rightarrow \min_{a,b}
$$

---

## 2. Минимизация функции двух переменных

Функция `S(a, b)` — это функция двух переменных `a` и `b`. Чтобы найти её минимум, нужно взять **частные производные** по `a` и `b`, приравнять их к нулю и решить систему уравнений.

### Частная производная по `a`:
$$
\frac{\partial S}{\partial a} = \frac{\partial}{\partial a} \sum_{i=1}^{n}(y_i - a x_i - b)^2
= \sum_{i=1}^{n} 2(y_i - a x_i - b)(-x_i)
= -2 \sum_{i=1}^{n} x_i (y_i - a x_i - b)
$$

Приравняем к нулю:

$$
\sum_{i=1}^{n} x_i (y_i - a x_i - b) = 0
\quad\text{(Уравнение 1)}
$$

---

### Частная производная по `b`:
$$
\frac{\partial S}{\partial b} = \frac{\partial}{\partial b} \sum_{i=1}^{n}(y_i - a x_i - b)^2
= \sum_{i=1}^{n} 2(y_i - a x_i - b)(-1)
= -2 \sum_{i=1}^{n} (y_i - a x_i - b)
$$

Приравняем к нулю:

$$
\sum_{i=1}^{n} (y_i - a x_i - b) = 0
\quad\text{(Уравнение 2)}
$$

## 3. Решение системы уравнений

Рассмотрим Уравнение 2:

$$
\sum_{i=1}^{n} (y_i - a x_i - b) = 0
\Rightarrow \sum y_i - a \sum x_i - n b = 0
\Rightarrow \frac{1}{n}\sum y_i - a \cdot \frac{1}{n}\sum x_i - b = 0
\Rightarrow \bar{y} - a \bar{x} - b = 0
\Rightarrow b = \bar{y} - a \bar{x}
$$

* Это уже наша **формула для `b`**

Теперь подставим это выражение в Уравнение 1:

$$
\sum_{i=1}^{n} x_i (y_i - a x_i - b) = 0
\Rightarrow \sum x_i (y_i - a x_i - (\bar{y} - a \bar{x})) = 0
$$

Раскрываем скобки:

$$
\sum x_i (y_i - a x_i - \bar{y} + a \bar{x}) = 0
\Rightarrow \sum x_i (y_i - \bar{y}) - a \sum x_i (x_i - \bar{x}) = 0
$$

Переносим `a` вправо:

$$
a \sum x_i (x_i - \bar{x}) = \sum x_i (y_i - \bar{y})
$$

Заметим, что:

- $$ \sum x_i (x_i - \bar{x}) = \sum (x_i - \bar{x})^2 $$
- $$ \sum x_i (y_i - \bar{y}) = \sum (x_i - \bar{x})(y_i - \bar{y}) $$
Итак, получаем:

$$
a = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
$$

## Финальные формулы

Вот они:

### Наклон прямой (коэффициент `a`):

$$
a = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
$$

### Свободный член `b`:

$$
b = \bar{y} - a \cdot \bar{x}
$$

### 1. Найдём средние значения x и y

$$\bar{x} = \frac{1}{n}\sum x_i,\quad \bar{y} = \frac{1}{n}\sum y_i
$$

Примерно получаем:

$$
\bar{x} \approx 4.46,\quad \bar{y} \approx 12.45
$$

### 2. Посчитаем числитель и знаменатель для `a`:

Числитель:

$$
\sum (x_i - \bar{x})(y_i - \bar{y}) \approx 219.3
$$

Знаменатель:

$$
\sum (x_i - \bar{x})^2 \approx 87.6
$$

Тогда:

$$
a \approx \frac{219.3}{87.6} \approx 2.50
$$

И далее:

$$
b = \bar{y} - a \cdot \bar{x} \approx 12.45 - 2.50 \cdot 4.46 \approx 1.3
$$

---

## Результат
Мы получаем:

$$
\hat{y} = 2.50 \cdot x + 1.3
$$

Что **совпадает** с исходной функцией!

---
#### Связанные темы [[Интерполяция]], [[Экстропаляция]], [[Апроксимация]]
#### Ссылки https://habr.com/ru/articles/514818/
