### Контекстные векторы слов

Идея - смысл слова опредедлляется контекстом.
построим векторы слов на основе контекста:

Пусть размер словарая у нас будет 50 тыс слов

Построи матрицу размерм 50 на 50 тыс(по строкам будет слова из словаря и по столбцам тоже будут слова из словаря, на пересечении i строки и  j столбца будет число отражающее частоту вхождения i слова в контексте с j словом, размер контекста определяется размером окна/window size).

Как эта таблица будет строитсья
Пусть у нас есть текст и мы будет идти по этому тексту методом скользящего окна. Пусть размер окна у нас будет 5, значит берем первые 5 слов теста, 3 слово у нас это target word а 2 сосоедних слова(слева и справа) являются контекстынми словами

После того как мы построили такую матрицу может считать что вектор каждого слова это вектор - строка/столбец данной матрицы(так как у нас матрица симетричная, не важно по строке или по столбцам).

#### плюсы:
Векторы(образованные данным способом) начинают отражать смысл слов! Их можно сравнивать на схожесть по расстоянию(cosine distance/MSE)

#### Недостатки:
Векторы все ееще доольно разряжны, требуют много лишней памяти
Размер словаря ограниен. Слова не попавшие в словарь не могут быть обработаны или нужно пересчитывать векторы и словать при добвалении новых слов


Для понижения размера/размерности матрицы можно использовать методы SVD разложеение матрицы
![[5298842543854838631.jpg]]

### Эмбеддинги слов

Все что мы делали до сих пор - на основе каки=-то соображеений строили векторы/матрицы слов/документов, оторые как-то отображали смысл слов =/документов.

А что сли попытаться выучить векторы слов/документов?

Что мы хотим:

Мы хотим выучить векторы слов небольшой размерности, которы отражали бы смысл слов:  их можно было бы сравнивать между собой с помощью неоторой метрики. Такие выучченные вектроы мы будет назвать эмбеддингами слов.

### Word2vec

Мы будем учить нейросеть по слову предсказывать слова, которые могут находиться в контексте (стоять вокруг этого слова).

Формализуем задачу:
Ставиться задача классификации
Количество классов - размер словаря n.
На вход нейросеть принимает слово, выдает n значении - распределение вероятности на слова в словаре.
Лосс-функция - кросс-энтропия меежду распределением, выданным сетью, и верным распраделенеим (one-ho вектором)


![[5298842543854838630.jpg]]

У нашей неросети в скрытом слое нету функции активации

![[5298842543854838629.jpg]]

Как происходит обучение нейросети:

Пусть у нас есть предложение: `[problems, turning, into, banking, crises, as, ...]`

Пусть сейчас target word=into

Получаем на выходе нейросети некоторое распределение вертяности, сравниваем его с правильным распределение вероятности, где правильно слово это problems

Следующие правильные/ождаемые распределения слов это слова turning, banking, crises. Далее мы двигаем наше окно на единицу и target word теперь у нас banking а ожидаемые распределения вероятностей для слов это распределеня слов: turning, into, crises  и тд

Давай тее пристальнее посмотреть на то что происходит внутри сети когда мы на вход ей подаем некоторое слово, пусть на том же сам примере слово into

Пусть into является i-ым словом в нашем словаре, подаем на вход сети one-hot vector где единственный 1 на i позиции(так как слово имеет i индекс в нашем словаре)

При подачее на вход сети этот вектор умножается на первую матрицу A и в результате него мы получаем матрицу, получается выход первого слоя сети это i строка матрицы A, потом этот i вектор подается на вход второго скрытого слоя(умножается на матрицу B).

![[5298842543854838627.jpg]]


Теперь давайте посмотрим на какой нибудь элемент выхода этого второго скрытого слоя, например на выход характеризующий слово banking - вектор слова на j столбце

вектор слова banking получился при помощи скалярного произведения строки i матрицы A(первый скрытый слои сети) на j столбец матриы B(второй скрытый слой сети). Тогда давай те считать строки матрицы A будем считать векторами слов(тоесть эмбеддингами слов из словаря).

Тоже самое можно проделать со строками матрицы B. Что мы получается делаем. В процессе обучения сети мы учим нейросеть на следующее. Она выучивает такие векторы слов(такие значения в матрицах A & B) что бы скалярное произведение между двух слов(двух вектором) которые находятся в контексте между собой ближе были как молжно больше. Тоесть чем ближе по контексту два слова(чем чащ встречаются два слова в одинаковых/близких контекстах) тем больше их скалярное произведение между i строкой матрицы A и j столбцом матрицы B.

![[5298842543854838626.jpg]]

Может возникнуть вопрос между тем что бы брать за эмбеддинги слов строки матрицы A или столбцы матрицы B. На самом дееле рабоатет то и то. Можно использовать и только одну матрицу и использовать ее как векторы эмбеддингов для слов. Но в данном случае мы используем две матрицы так как одну из них можно интерпритировать как векторы для эмбеддинга центральных слов а другую для эмбеддингов для слоов контекстов.  (Когда вы обучаете моделья по слову предсказывать контекст у вам матрица A выступает как векторы для эмбеддингов центральных слов а матрица B как векторы для эмбеддингов слов контекстов а при обучении модели по контексту предсказывать слово роль этих матриц отзеркаливается).

После обуччения сети мы получаем векторы размера k для всех слов в словар. Размер k мы можем задавать сами.

Эти векторы содержат смысл слов. Их можно сравнивать между собой с помощью косинусного расстояния. 
