**TF-IDF (Term Frequency-Inverse Document Frequency)** — это статистическая мера, используемая для оценки важности слова в контексте документа, являющегося частью коллекции документов или корпуса. Мера TF-IDF часто используется в задачах обработки естественного языка и информационного поиска для взвешивания слов в документах.

### Основные компоненты TF-IDF

**TF (Term Frequency):**

- **Частота термина (Term Frequency)** — это количество раз, которое слово встречается в документе. Чем чаще слово встречается в документе, тем выше его TF.

Формула:
$$ TF(t,d)=\frac {\text{Количество вхождений слова t в документ d} } {\text{Общее количество слов в документе}} $$ 
​**IDF (Inverse Document Frequency):**

- **Обратная частота документа (Inverse Document Frequency)** — это мера того, насколько редко слово встречается во всей коллекции документов. Чем реже слово встречается в коллекции, тем выше его IDF.

Формула:
$$  IDF(t,D) = log(\frac{Общее количество документов в коллекции D​} {Количество документов, содержащих слово t}) $$

**TF-IDF:**

- **TF-IDF** — это произведение TF и IDF. Оно позволяет оценить важность слова в контексте конкретного документа и всей коллекции документов.

Формула:
$$ \text{TF-IDF(t,d,D)}=TF(t,d)×IDF(t,D) $$

### Пример

Предположим, у нас есть следующие документы:

1. "машина едет по дороге"
2. "самолет летит по небу"
3. "машина и самолет"

**Шаги вычисления TF-IDF для слова "машина":**

1. **TF (Term Frequency):**
    
    - В первом документе: TF("машина", D1) = 1/4 = 0.25
    - Во втором документе: TF("машина", D2) = 0
    - В третьем документе: TF("машина", D3) = 1/2 = 0.5
2. **IDF (Inverse Document Frequency):**
    
    - Общее количество документов: 3
    - Количество документов, содержащих слово "машина": 2
    - IDF("машина", D) = log(3/2) ≈ 0.176
3. **TF-IDF:**
    
    - В первом документе: TF-IDF("машина", D1, D) = 0.25 * 0.176 ≈ 0.044
    - Во втором документе: TF-IDF("машина", D2, D) = 0 * 0.176 = 0
    - В третьем документе: TF-IDF("машина", D3, D) = 0.5 * 0.176 ≈ 0.088

### Зачем использовать TF-IDF?

- **Взвешивание слов:** TF-IDF позволяет придавать больший вес более значимым словам в документе, которые редко встречаются в других документах.
- **Уменьшение влияния стоп-слов:** Часто встречающиеся слова (например, "и", "в", "на") имеют низкий IDF и, следовательно, низкий TF-IDF, что уменьшает их влияние на результаты.
- **Улучшение качества поиска:** В системах информационного поиска TF-IDF помогает ранжировать документы по релевантности запросу.

### Применение в scikit-learn

В библиотеке scikit-learn TF-IDF можно вычислить с помощью класса `TfidfVectorizer`:

```python
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# Пример текстов
texts = [
    "машина едет по дороге",
    "самолет летит по небу",
    "машина и самолет"
]

# Создаем экземпляр TfidfVectorizer
vectorizer = TfidfVectorizer()
# vectorizer = TfidfVectorizer(ngram_range=(2, 2))  # (2, 2) означает использование только биграмм (пары слов).

# Преобразуем тексты в матрицу TF-IDF
tfidf_matrix = vectorizer.fit_transform(texts)

# Получаем список слов и биграмм
feature_names = vectorizer.get_feature_names_out()

# Преобразуем матрицу TF-IDF в DataFrame для удобства
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)

# Выводим матрицу TF-IDF
print(tfidf_matrix.toarray())

# Выводим DataFrame
print(tfidf_df)
```